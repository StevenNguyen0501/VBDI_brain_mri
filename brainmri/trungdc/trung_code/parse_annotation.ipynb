{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "# import pandas_read_xml as pdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(xml_path):\n",
    "    \"\"\"\n",
    "    Process xml annotation file\n",
    "    input: directory to xml file\n",
    "    output: return required field in a list. The following fields in added in addition to field in process_label function: \n",
    "    date, patientid, patientPid, diagnosis\n",
    "\n",
    "    NOTE: role of diagnosis column is unclear\n",
    "    \"\"\"\n",
    "    \n",
    "    # read xml file\n",
    "    df = pdx.read_xml(xml_path, [\"study_query\"])\n",
    "    \n",
    "    # extract date\n",
    "    date = df[\"study\"][\"@date\"]\n",
    "\n",
    "    # extract patientid\n",
    "    patientid = df[\"study\"][\"patient\"][\"@id\"]\n",
    "    patientPid = df[\"study\"][\"patient\"][\"@pid\"]\n",
    "\n",
    "    #extract diagnosis for each tag\n",
    "    diagnosis = [ele[\"id\"] for ele in df[\"study\"][\"diagnosis\"]]  \n",
    "\n",
    "    # extract every other info\n",
    "    out = [process_label(label) for label in df[\"study\"][\"label\"]]\n",
    "\n",
    "    rows = []\n",
    "    # append annotations\n",
    "    for i in out:\n",
    "        row = [date, patientid, patientPid, diagnosis] + i\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_label(label):\n",
    "    \"\"\"\n",
    "    Process each <label> tag inside each xml file\n",
    "    input: <label> tag in form of orderdict\n",
    "    output: extract the following fields: \n",
    "    sessionid, type_anot, annotation, scope, unit, timestamp, imageuid, seriesuid, studyuid, tag, points\n",
    "\n",
    "    NOTE: each xml file has several <label> tag\n",
    "    \"\"\"\n",
    "\n",
    "    sessionid = label[\"@sessionId\"]\n",
    "    type_anot = label[\"@type\"]\n",
    "    annotation = label[\"@annotation\"]\n",
    "    scope = label[\"@scope\"]\n",
    "    unit = label[\"@pointUnit\"]\n",
    "    timestamp = label[\"@createTimestamp\"]\n",
    "    imageuid = label[\"@imageUid\"]\n",
    "    seriesuid = label[\"@seriesUid\"]\n",
    "    studyuid = label[\"@studyUid\"]\n",
    "\n",
    "    #NOTE some file has noisy <tag>\n",
    "    # only keep tumor relate to brain\n",
    "    \n",
    "\n",
    "    if len(label[\"tags\"][\"value\"]) > 1:\n",
    "        tag = []\n",
    "        for sub_tag in label[\"tags\"][\"value\"]:\n",
    "            tag.append(sub_tag[\"@name\"])\n",
    "    else:\n",
    "        tag = [label[\"tags\"][\"value\"][\"@name\"]]\n",
    "    \n",
    "    # if type_anot == \"global\":\n",
    "    #     print(\"Before\", tag)\n",
    "        # tag = list(set(tag) & check_label)\n",
    "    #     print(\"After\", tag)\n",
    "    #     if (len(tag) > 1) & (\"Other tumor\" in tag) :\n",
    "    #         print(tag, \"Type 1\")\n",
    "    #     elif (len(tag) > 1) & (\"Other tumor\" not in tag):\n",
    "    #         print(tag, \"Type 2\")\n",
    "\n",
    "    #NOTE file 95, 111, 120 has error value in <point>\n",
    "    if label[\"point\"] != None:\n",
    "        if len(label[\"point\"][\"value\"]) == 12:\n",
    "            points = [[float(point[\"@x\"]), float(point[\"@y\"]), float(point[\"@z\"])] for point in label[\"point\"][\"value\"]]\n",
    "        else: \n",
    "            points = None\n",
    "    else:\n",
    "        points = None\n",
    "\n",
    "    return [sessionid, type_anot, annotation, scope, unit, timestamp, imageuid, seriesuid, studyuid, tag, points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_xml(root_folder_path, to_csv = False, to_pickle = False, file_name = \"\"):\n",
    "    \"\"\"\n",
    "    combine data from xml files into dataframe and\n",
    "    input: \n",
    "    - root_folder_path: directory to folder contain xml files, \n",
    "    - to_csv, to_pickle: set to True if you want \n",
    "    - file_name: name of csv/pickle file in string without .csv/.pkl (optional but required when to_csv/to_pickle is TRUE)\n",
    "    \"\"\"\n",
    "    print(\"Tags that has multiple value:\")\n",
    "\n",
    "    study_folder = [os.path.join(root_folder_path, i) for i in os.listdir(root_folder_path)]\n",
    "    xml_paths = []\n",
    "    for subdir in study_folder:\n",
    "        for label_name in os.listdir(subdir):\n",
    "            xml_path = os.path.join(subdir, label_name)\n",
    "            xml_paths.append(xml_path)\n",
    "\n",
    "    # add row from each xml file to datarfame\n",
    "    df_rows = []\n",
    "    for xml_path in xml_paths:\n",
    "        xml_out = process_xml(xml_path)\n",
    "        df_rows += xml_out\n",
    "\n",
    "    # convert to dataframe\n",
    "    df_out = pd.DataFrame(df_rows, columns = [\"date\", \"patientid\", \"patientPid\", \"diagnosis\",\"sessionid\", \"type_anot\", \"annotation\", \"scope\", \"unit\", \"timestamp\", \"imageuid\", \"seriesuid\", \"studyuid\", \"tag\", \"points\"])\n",
    "\n",
    "    drop_index = []\n",
    "    for index, row in df_out.iterrows():\n",
    "        if (row[\"points\"] == None) & (row[\"type_anot\"] == \"local\"):\n",
    "            drop_index.append(index)\n",
    "    df_out = df_out.drop(drop_index, axis = 0)\n",
    "    df_out.reset_index(drop = True)\n",
    "\n",
    "    # summary output\n",
    "    print(\"\\n\")\n",
    "    print(\"Number of xml files(study):\", len(xml_paths))\n",
    "    print(\"Number of annotated lession:\", len(df_rows))\n",
    "    print(f\"Number of global annotation:\", len(df_out[df_out.type_anot == 'global']))\n",
    "    print(f\"Number of local annotation:\", len(df_out[df_out.type_anot == 'local']))\n",
    "\n",
    "    # write to file\n",
    "    if (to_csv & (file_name != \"\")):\n",
    "        df_out.to_csv(file_name + \".csv\")\n",
    "    if (to_pickle & (file_name != \"\")):\n",
    "        df_out.to_pickle(file_name + \".pkl\")\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tags that has multiple value:\n\n\nNumber of xml files(study): 148\nNumber of annotated lession: 515\nNumber of global annotation: 155\nNumber of local annotation: 321\n"
     ]
    }
   ],
   "source": [
    "root_folder_path = \"/home/single1/BACKUP/tintrung/brain-mri-tumor-xml\"\n",
    "out = process_dataset_xml(root_folder_path, to_csv=True, to_pickle=True, file_name=\"summary_anot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_label = set([\"Other tumor\", \"Glioma\", \"Meningioma\", \"Pituitary adenoma\", \"Cerebral tumor\", \"Neurinoma\", \"Cavernoma\",\"Lymphoma\",\"Chordoma\"])\n",
    "# out[out.type_anot == \"global\"][\"tag\"]\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "uid = out.iloc[14][\"seriesuid\"]\n",
    "# out[out.seriesuid == uid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "476\nMass/Nodule                   155\nCerebral edema                 74\nSinus lesion                   24\nMidline shift                  21\nCyst component                 14\nIschemia                       12\nVentricular dilation            8\nHemorrhagic component           7\nCavernoma                       6\nDemyelination                   5\nSubdural effusion               4\nCSF-like lesion                 4\nBone lesion                     3\nMass effect                     2\nOther lesion                    2\nIntracranial herniation         1\nArteriovenous malformation      1\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "drop_index = []\n",
    "for index, row in out.iterrows():\n",
    "    if (row[\"points\"] == None) & (row[\"type_anot\"] == \"local\"):\n",
    "        drop_index.append(index)\n",
    "out = out.drop(drop_index, axis = 0)\n",
    "out.reset_index(drop = True)\n",
    "print(len(out))\n",
    "\n",
    "localtag = []\n",
    "for i in out[out.type_anot == \"local\"][\"tag\"]:\n",
    "    localtag += i\n",
    "localtag = pd.Series(localtag).value_counts()\n",
    "print(localtag)\n",
    "\n",
    "localtagunique = []\n",
    "for  i in localtag.keys():\n",
    "    localtagunique.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Mass/Nodule'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "localtagunique[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(localtagunique)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'summary_annot.pkl'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6f05eecad11d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"summary_annot.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \"\"\"\n\u001b[0;32m    184\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m     with get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m             \u001b[1;31m# Binary mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'summary_annot.pkl'"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"summary_anot.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd0ffb59775285fb50ec2bce28627f6615434f720787f620151e8872e04c1848588",
   "display_name": "Python 3.9.4 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}